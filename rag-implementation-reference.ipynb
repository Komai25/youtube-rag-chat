{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dafd9771",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6a15804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Komai\\Desktop\\RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "import tempfile\n",
    "import whisper\n",
    "from pytube import YouTube\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "YOUTUBE_VIDEO = \"https://youtu.be/T-D1OfcDW1M\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "597edec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are you today? Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "model = Ollama(model=\"hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF\")\n",
    "\n",
    "response = model.invoke(\"hi\")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "427ef43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How are you today? Is there something I can help you with or would you like to chat?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser \n",
    "chain.invoke('Hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b8d39fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: \\nYou are a helpful assistant. Use the provided context to answer the question as accurately as possible.\\nIf the answer cannot be found in the context, respond with \"I don\\'t know\".\\n\\nContext:\\nKomai\\'s brother is Obai\\n\\nQuestion:\\nWho is Komai\\'s brother?\\n\\nAnswer:'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "You are a helpful assistant. Use the provided context to answer the question as accurately as possible.\n",
    "If the answer cannot be found in the context, respond with \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "prompt.format(context=\"Komai's brother is Obai\", question=\"Who is Komai's brother?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1457794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Obai'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\n",
    "    \"context\": \"Komai's brother is Obai\",\n",
    "    \"question\": \"Who is Komai's brother?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a279fd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://youtu.be/T-D1OfcDW1M\n",
      "[youtube] T-D1OfcDW1M: Downloading webpage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Unable to download webpage: HTTP Error 429: Too Many Requests (caused by <HTTPError 429: Too Many Requests>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] T-D1OfcDW1M: Downloading initial data API JSON\n",
      "[youtube] T-D1OfcDW1M: Downloading tv client config\n",
      "[youtube] T-D1OfcDW1M: Downloading player 4b357d1b-tv\n",
      "[youtube] T-D1OfcDW1M: Downloading tv player API JSON\n",
      "[youtube] T-D1OfcDW1M: Downloading ios player API JSON\n",
      "[youtube] T-D1OfcDW1M: Downloading web player API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] T-D1OfcDW1M: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] T-D1OfcDW1M: Downloading m3u8 information\n",
      "[info] Testing format 234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[download] Got error: HTTP Error 403: Forbidden\n",
      "WARNING: Unable to download format 234. Skipping...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Testing format 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[download] Got error: HTTP Error 403: Forbidden\n",
      "WARNING: Unable to download format 233. Skipping...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] T-D1OfcDW1M: Downloading 1 format(s): 18\n",
      "[download] Destination: audio.mp4\n",
      "[download] 100% of   10.23MiB in 00:00:08 at 1.25MiB/s   \n",
      "[ExtractAudio] Destination: audio.mp3\n",
      "Deleting original file audio.mp4 (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "import yt_dlp\n",
    "\n",
    "output_path = \"audio.%(ext)s\"  \n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'outtmpl': output_path,\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'mp3',\n",
    "        'preferredquality': '192',\n",
    "    }],\n",
    "}\n",
    "\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([YOUTUBE_VIDEO])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75b7043f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1.42G/1.42G [08:55<00:00, 2.85MiB/s]\n"
     ]
    }
   ],
   "source": [
    "# Whisper model ( \"base\", \"small\", \"medium\", or \"large\")\n",
    "model = whisper.load_model(\"medium\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ee15fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Komai\\Desktop\\RAG\\.venv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Large language models, they are everywhere. They get some things amazingly right and other things very interestingly wrong. My name is Marina Danilevsky. I am a senior research scientist here at IBM Research, and I want to tell you about a framework to help large language models be more accurate and more up-to-date. Retrieval Augmented Generation, or RAG. Let's just talk about the generation part for a minute. So, forget the retrieval augmented. So, the generation, this refers to large language models, or LLMs, that generate text in response to a user query referred to as a prompt. These models can have some undesirable behavior. I want to tell you an anecdote to illustrate this. So, my kids, they recently asked me this question. In our solar system, what planet has the most moons? And my response was, oh, that's really great that you're asking me this question. I loved space when I was your age. Of course, that was like 30 years ago. But I know this. I read an article, and the article said that it was Jupiter and 88 moons. So, that's the answer. Now, actually, there's a couple of things wrong with my answer. First of all, I have no source to support what I'm saying. So, even though I confidently said, I read an article, I know the answer, I'm not sourcing it. I'm giving the answer off the top of my head. And also, I actually haven't kept up with this for a while, and my answer is out of date. So, we have two problems here. One is no source. And the second problem is that I am out of date. And these, in fact, are two behaviors that are often observed as problematic when interacting with large language models. They are LLM challenges. Now, what would have happened if I had taken a beat and first gone and looked up the answer on a reputable source like NASA? Well, then I would have been able to say, ah, okay, so the answer is Saturn with 146 moons. And in fact, this keeps changing because scientists keep on discovering more and more moons. So, I have now grounded my answer in something more believable. I have not hallucinated or made up an answer. Oh, and by the way, I didn't leak personal information about how long ago it's been since I was obsessed with space. All right, so what does this have to do with large language models? Well, how would a large language model have answered this question? So, let's say that I have a user asking this question about moons. A large language model would confidently say, okay, I have been trained, and from what I know in my parameters during my training, the answer is Jupiter. The answer is wrong, but we don't know. The large language model is very confident in what it answered. Now, what happens when you add this retrieval augmented part here? What does that mean? That means that now, instead of just relying on what the LLM knows, we are adding a content store. This could be open, like the internet. This could be closed, like some collection of documents, collection of policies, whatever. The point, though, now, is that the LLM first goes and talks to the content store and says, hey, can you retrieve from me information that is relevant to what the user's query was? And now, with this retrieval augmented answer, it's not Jupiter anymore. We know that it is Saturn. What does this look like? Well, first, user prompts the LLM with their question. They say this is what my question was. And originally, if we're just talking to a generative model, the generative model says, oh, okay, I know the response. Here it is, here's my response. But now, in the RAB framework, the generative model actually has an instruction that says, no, no, no. First, go and retrieve relevant content. Combine that with the user's question, and only then generate the answer. So, the prompt now has three parts. The instruction to pay attention to the retrieved content, together with the user's question, now give a response, and in fact, now you can give evidence for why your response was what it was. So, now hopefully you can see, how does RAB help the two LLM challenges that I had mentioned before? So, first of all, I'll start with the out-of-date part. Now, instead of having to retrain your model if new information comes up, like, hey, we found some more moons. Now, it's a Jupiter again. Maybe it'll be Saturn again in the future. All you have to do is you automate your data store with new information, updated information. So, now the next time that a user comes and asks the question, we're ready. We just go ahead and retrieve the most up-to-date information. The second problem, source. Well, the LLM model is now being instructed to pay attention to primary source data before giving its response, and in fact, now being able to give evidence. This makes it less likely to hallucinate or to leak data because it is less likely to rely only on information that it learned during training. It also allows us to get the model to have a behavior that can be very positive, which is knowing when to say, I don't know. If the user's question cannot be reliably answered based on your data store, the model should say, I don't know, instead of making up something that is believable and may mislead the user. This can have a negative effect as well, though, because if the retriever is not sufficiently good to give the large language model the best, most highest quality grounding information, then maybe the user's query that is answerable doesn't get an answer. So, this is actually why lots of folks, including many of us here at IBM, are working the problem on both sides. We are both working to improve the retriever to give the large language model the best quality data on which to ground its response, and also the generative part, so that the LLM can give the richest, best response, finally, to the user when it generates the answer. Thank you for learning more about RAG, and like and subscribe to the channel. Thank you. Thank you.\n"
     ]
    }
   ],
   "source": [
    "# Transcribe the audio file\n",
    "result = model.transcribe(\"audio.mp3\")\n",
    "\n",
    "with open(\"transcription.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result[\"text\"])\n",
    "\n",
    "# Print the transcript\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5800e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clear whisper cache\n",
    "\n",
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# cache_dir = os.path.expanduser(\"~/.cache/whisper\")\n",
    "# if os.path.exists(cache_dir):\n",
    "#     shutil.rmtree(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8687e80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Komai\\AppData\\Local\\Temp\\ipykernel_10644\\1247942452.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedder = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n"
     ]
    }
   ],
   "source": [
    "embedder = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cdd80c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Komai\\Desktop\\RAG\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "عدد القطع الدلالية: 5\n",
      " Large language models, they are everywhere. They get some things amazingly right and other things very interestingly wrong. My name is Marina Danilevsky. I am a senior research scientist here at IBM Research, and I want to tell you about a framework to help large language models be more accurate and more up-to-date. Retrieval Augmented Generation, or RAG. Let's just talk about the generation part for a minute. So, forget the retrieval augmented. So, the generation, this refers to large language models, or LLMs, that generate text in response to a user query referred to as a prompt. These models can have some undesirable behavior.\n"
     ]
    }
   ],
   "source": [
    "chunker = SemanticChunker(\n",
    "    embeddings=embedder,\n",
    "    buffer_size=1,\n",
    "    breakpoint_threshold_type=\"percentile\",\n",
    "    breakpoint_threshold_amount=95.0, \n",
    "    min_chunk_size=50 \n",
    ")\n",
    "\n",
    "raw = open(\"transcription.txt\", encoding=\"utf-8\").read()\n",
    "docs = [Document(page_content=raw)]\n",
    "\n",
    "# تقسيم دلالي\n",
    "semantic_chunks = chunker.create_documents([raw])  # أو split_documents(docs)\n",
    "\n",
    "print(f\"عدد القطع الدلالية: {len(semantic_chunks)}\")\n",
    "print(semantic_chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "880dd013",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(semantic_chunks,embedding=embedder)\n",
    "db.save_local(\"faiss.index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5719c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_context(question):\n",
    "    return \"\\n\\n\".join([\n",
    "        doc.page_content for doc in db.similarity_search(question, k=4)\n",
    "    ])\n",
    "\n",
    "setup = RunnableParallel({\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": fetch_context\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee63997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = setup | prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32c892e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Komai\\Desktop\\RAG\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Large language models (LLMs) are a type of artificial intelligence that can process and generate human-like text based on input from users, such as questions or prompts. They have been trained on vast amounts of data and use complex algorithms to understand the context and intent behind the user's query.\\n\\nIn this specific scenario, Large Language Models get some things amazingly right and other things very interestingly wrong. The question was about what Large Language Models are, but the response didn't accurately convey their capabilities or limitations.\\n\\nI don't know.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What are Large Language Models?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bc2462a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Komai\\Desktop\\RAG\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Saturn.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"what planet has the most moons?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
